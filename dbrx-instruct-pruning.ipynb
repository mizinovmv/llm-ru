{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8663d32-626f-404b-b7ab-0b69891ac05d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aee5ab7-cda5-4898-94be-b95629f0ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.1.2+cu121\n",
      "transformers 4.39.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"transformers\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0091bba4-6fc4-4aa8-bfc3-83e5b3c2646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_seed = 14\n",
    "\n",
    "model_id = \"databricks/dbrx-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e6dcd2-98da-4eeb-a551-4c69921f706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TiktokenTokenizerWrapper(name_or_path='databricks/dbrx-instruct', vocab_size=100277, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t100257: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100277: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100278: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100279: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e12d74-94b6-46bb-88e9-7323fe472060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 2010\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "# sharegpt format\n",
    "test_dataset = Dataset.from_parquet(\"dbrx_instruct_pruning_test_dataset.parquet\")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82105277-0ff5-422b-abd6-e632f5dafd1d",
   "metadata": {},
   "source": [
    "## chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d101e3-438d-437c-ad4e-b2a1a1498d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "\n",
    "def preprocess_chat_template(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False):\n",
    "    conv_roles = [\"system\", \"user\", \"assistant\"]\n",
    "    roles = {\"system\": conv_roles[0], \"human\": conv_roles[1], \"gpt\": conv_roles[2]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            # assert role == conv_roles[j % 2], f\"{i}\"\n",
    "            messages.append({\"role\": role, \"content\": sentence[\"value\"]})\n",
    "        conversations.append(tokenizer.apply_chat_template(messages, tokenize=False, add_special_tokens=False))\n",
    "\n",
    "    if has_image:\n",
    "        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            conversations,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    # Mask targets\n",
    "    sep = \"<|im_start|>assistant\\n\"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        conv_sep = \"<|im_end|>\\n\"\n",
    "        rounds = conversation.split(conv_sep)\n",
    "        cur_len = 0\n",
    "        instruction_len = 0\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "            if sep not in rou:\n",
    "                if has_image:\n",
    "                    instruction_len = len(tokenizer_image_token(rou + conv_sep, tokenizer))\n",
    "                else:\n",
    "                    instruction_len = len(tokenizer(rou + conv_sep, add_special_tokens=False).input_ids)\n",
    "                target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "            cur_len += instruction_len\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2197adaf-03ad-43af-abca-c27b2dd98da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: <|im_start|> system\n",
      "You are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\n",
      "YOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\n",
      "You assist with various tasks, from writing to coding (using markdown for code blocks — remember to use ``` with code, JSON, and tables).\n",
      "(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\n",
      "This is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\n",
      "YOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER'S QUERY. <|im_end|> \n",
      " <|im_start|> user\n",
      "Question: The most likely reason sound travels faster in saltwater than in freshwater is that saltwater\n",
      "A) is more elastic.\n",
      "B) absorbs heat faster.\n",
      "C) has a higher density.\n",
      "D) reflects sound better.\n",
      "\n",
      "Answer: <|im_end|> \n",
      " <|im_start|> assistant\n",
      "C <|im_end|>\n",
      "\n",
      "label <|im_start|> assistant\n",
      "C <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "sources = test_dataset[1][\"conversations\"]\n",
    "\n",
    "item = preprocess_chat_template([sources], tokenizer)\n",
    "\n",
    "print(\"input:\", tokenizer.decode(item['input_ids'][item['input_ids'].ne(IMAGE_TOKEN_INDEX)]))\n",
    "print()\n",
    "print(\"label\", tokenizer.decode(item['labels'][item['labels'].ne(IGNORE_INDEX | IMAGE_TOKEN_INDEX)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585cd847-dd21-450c-be0a-a6e279d7960c",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc2564f-82fe-419b-85d3-f0e7eabcf0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.1.2+cu121\n",
      "transformers 4.39.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51bc8dd22354428989dcfab9f99293d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DbrxForCausalLM(\n",
       "  (transformer): DbrxModel(\n",
       "    (wte): Embedding(100352, 6144)\n",
       "    (blocks): ModuleList(\n",
       "      (0-39): 40 x DbrxBlock(\n",
       "        (norm_attn_norm): DbrxNormAttentionNorm(\n",
       "          (norm_1): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): DbrxFlashAttention2(\n",
       "            (Wqkv): Linear(in_features=6144, out_features=8192, bias=False)\n",
       "            (out_proj): Linear(in_features=6144, out_features=6144, bias=False)\n",
       "            (rotary_emb): DbrxRotaryEmbedding()\n",
       "          )\n",
       "          (norm_2): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): DbrxFFN(\n",
       "          (router): DbrxRouter(\n",
       "            (layer): Linear(in_features=6144, out_features=16, bias=False)\n",
       "          )\n",
       "          (experts): DbrxExperts(\n",
       "            (mlp): DbrxExpertGLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=6144, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"transformers\", transformers.__version__)\n",
    "\n",
    "device_map = \"auto\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32535d4-93a2-42e6-afbd-2d05e5db8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Union\n",
    "\n",
    "class DbrxFFNPruningWrapper(nn.Module):\n",
    "    def __init__(self, model, r = None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.top_experts_list = []\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        weights, top_weights, top_experts = self.model.router(x)\n",
    "        out = self.model.experts(x, weights, top_weights, top_experts)\n",
    "        self.top_experts_list.append(top_experts.detach().to('cpu', non_blocking=True))\n",
    "        return out, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3818b92c-856e-4396-b381-ad5109a6b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in model.transformer.blocks:\n",
    "    block.ffn = DbrxFFNPruningWrapper(block.ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad077f8-245e-4177-9afc-72951d225951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "with torch.inference_mode():\n",
    "    hidden_states = {}\n",
    "    model.transformer.wte = model.transformer.wte.to(\"cuda\")\n",
    "    for i, sources in tqdm(enumerate(test_dataset[\"conversations\"])):\n",
    "        item = preprocess_chat_template([sources], tokenizer)\n",
    "        inputs_embeds = model.transformer.wte.forward(item['input_ids'].to(\"cuda\"))\n",
    "\n",
    "        # hidden_states = inputs_embeds\n",
    "        hidden_states[i] = inputs_embeds.detach().to('cpu', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9d222-bf94-4d01-9917-3d17d502c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "def release_list(a):\n",
    "   del a[:]\n",
    "   del a\n",
    "\n",
    "layer_dict = []\n",
    "with torch.inference_mode():\n",
    "    for block in tqdm(model.transformer.blocks):\n",
    "        block = block.to(\"cuda\")\n",
    "        for i, sources in tqdm(enumerate(test_dataset[\"conversations\"])):\n",
    "            past_seen_tokens = 0\n",
    "            block_outputs = block(\n",
    "                hidden_states[i].to(\"cuda\"),\n",
    "                torch.arange(  # type: ignore\n",
    "                        past_seen_tokens,\n",
    "                        past_seen_tokens + hidden_states[i].shape[1],\n",
    "                        device=\"cuda\").unsqueeze(0).to(\"cuda\")\n",
    "            )\n",
    "            hidden_states[i] = block_outputs[0].detach().to('cpu', non_blocking=True)\n",
    "\n",
    "        layer_nums = []\n",
    "        for x in block.ffn.top_experts_list[0]:\n",
    "            for v in x.tolist():\n",
    "                layer_nums.append(int(v))\n",
    "        layer_nums = pd.Series(layer_nums)\n",
    "        layer_dict.append(layer_nums.value_counts().reset_index())\n",
    "        release_list(block.ffn.top_experts_list)\n",
    "\n",
    "        block.to(\"meta\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bd295e9-71ad-467b-90fb-eadd568846f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(layer_dict, \"dbrx_instruct_pruning_layer_stats.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8afc5f-5a98-4633-b7e2-87b4a25b063f",
   "metadata": {},
   "source": [
    "# Model pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b8b96cc-130b-4292-9c86-dde7dfad438a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  count\n",
       "0       7    437\n",
       "1      15    395\n",
       "2       0    308\n",
       "3       8    293\n",
       "4       4    248\n",
       "5      13    243\n",
       "6       5    183\n",
       "7       3    177\n",
       "8      14    173\n",
       "9      11    165\n",
       "10      1    161\n",
       "11      6    159\n",
       "12     10    151\n",
       "13     12    148\n",
       "14      9    113\n",
       "15      2     54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbrx_ru_pruning_layer_stats = torch.load(\"dbrx_instruct_pruning_layer_stats.pt\")\n",
    "dbrx_ru_pruning_layer_stats[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e2b32d7-b05c-462c-a026-cc134ffa6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_dir = hf_hub_download(repo_id=model_id, filename=\"modeling_dbrx.py\")\n",
    "model_dir = Path(model_dir).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8af456a8-7ca2-4b2c-9a94-8f1292471ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/databricks/dbrx-instruct/discussions/10\n",
    "\n",
    "NUM_EXPERTS = 16\n",
    "HIDDEN_SIZE = 6144\n",
    "FFN_HIDDEN_SIZE = 10752\n",
    "\n",
    "TARGET_NUM_EXPERTS = 8\n",
    "\n",
    "def change_tensor(tensor, reverse=False):\n",
    "    output = [x.contiguous() if not reverse else x.t().contiguous() for x in tensor.reshape(NUM_EXPERTS, FFN_HIDDEN_SIZE, HIDDEN_SIZE)]\n",
    "    return output\n",
    "\n",
    "def change_mlp(tensors):\n",
    "    keys = list(tensors.keys())\n",
    "    for k in keys:\n",
    "        if \"router\" in k:\n",
    "            block_idx = int(k.rsplit('.')[2])\n",
    "            name = f\"transformer.blocks.{block_idx}.ffn.router.layer.weight\"\n",
    "            experts_to_reserve = sorted(list(dbrx_ru_pruning_layer_stats[block_idx][:TARGET_NUM_EXPERTS][\"index\"].values))\n",
    "            tensor = tensors.pop(k)\n",
    "            t = tensor[experts_to_reserve]\n",
    "            tensors[name] = t\n",
    "        if any([x in k for x in ['w1', 'v1', 'w2']]):\n",
    "            prefix,dtype = k.rsplit('.', 1)\n",
    "\n",
    "            block_idx = int(prefix.rsplit('.')[2])\n",
    "            experts_to_reserve = sorted(list(dbrx_ru_pruning_layer_stats[block_idx][:TARGET_NUM_EXPERTS][\"index\"].values))\n",
    "\n",
    "            tensor = tensors.pop(k)\n",
    "            output_tensor = change_tensor(tensor, dtype=='w2')\n",
    "            for i in range(TARGET_NUM_EXPERTS):\n",
    "                t = output_tensor[experts_to_reserve[i]]\n",
    "                name = f'{prefix}.{i}.{dtype}.weight'\n",
    "                tensors[name] = t\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f6374-37e3-47d9-af49-4fc25798ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./dbrx-instruct-8X\").absolute()\n",
    "\n",
    "for file in model_dir.glob('*.safetensors'):\n",
    "    print(file)\n",
    "    tensors = {}\n",
    "    with safe_open(file, 'pt') as f:\n",
    "        metadata = f.metadata()\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    tensors = change_mlp(tensors)\n",
    "    save_file(tensors, (output_dir / file.name).as_posix(), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3f8f48b4-8480-41b7-9d35-e01225567717",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_dir / 'model.safetensors.index.json') as f:\n",
    "    weight_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "884f4371-526a-4597-9538-00a39ec5d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_keys = list(weight_map['weight_map'])\n",
    "for k in weight_keys:\n",
    "    if any([x in k for x in ['w1', 'v1', 'w2']]):\n",
    "        prefix,dtype = k.rsplit('.', 1)\n",
    "        value = weight_map['weight_map'].pop(k)\n",
    "        for i in range(TARGET_NUM_EXPERTS):\n",
    "            weight_map['weight_map'][f'{prefix}.{i}.{dtype}.weight'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c68e753-88ce-4ab3-8bb9-3fea44b071dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131596523520"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_map[\"metadata\"]['total_size'] = int(weight_map[\"metadata\"]['total_size'] / (NUM_EXPERTS / TARGET_NUM_EXPERTS))\n",
    "weight_map[\"metadata\"]['total_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "62243b85-2f2b-44ea-b112-bc4197653c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_map = sorted(list(weight_map['weight_map'].items()))\n",
    "weight_map['weight_map'] = dict(sorted_map)\n",
    "\n",
    "with open(output_dir / 'model.safetensors.index.json', 'w') as f:\n",
    "    json.dump(weight_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e83793-8341-4849-9f53-6a636dac076d",
   "metadata": {},
   "source": [
    "# Model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1720e1c-a9bb-4dcf-a904-6205a8619f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"dbrx-instruct-8X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584e69d7-065b-48e7-852a-e65adb8188bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TiktokenTokenizerWrapper(name_or_path='dbrx-instruct-8X', vocab_size=100277, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|im_end|>', 'unk_token': '<|im_end|>', 'pad_token': '<|pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t100257: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100277: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100278: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100279: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# fix 'eos_token': '<|im_end|>'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce26d5a-647c-4965-a151-e02e9868af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.1.2+cu121\n",
      "transformers 4.39.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3d292e1c21457882cf9ac60f29751c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DbrxForCausalLM(\n",
       "  (transformer): DbrxModel(\n",
       "    (wte): Embedding(100352, 6144)\n",
       "    (blocks): ModuleList(\n",
       "      (0-39): 40 x DbrxBlock(\n",
       "        (norm_attn_norm): DbrxNormAttentionNorm(\n",
       "          (norm_1): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): DbrxFlashAttention2(\n",
       "            (Wqkv): Linear4bit(in_features=6144, out_features=8192, bias=False)\n",
       "            (out_proj): Linear4bit(in_features=6144, out_features=6144, bias=False)\n",
       "            (rotary_emb): DbrxRotaryEmbedding()\n",
       "          )\n",
       "          (norm_2): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): DbrxFFN(\n",
       "          (router): DbrxRouter(\n",
       "            (layer): Linear4bit(in_features=6144, out_features=8, bias=False)\n",
       "          )\n",
       "          (experts): DbrxExperts(\n",
       "            (mlp): ModuleList(\n",
       "              (0-7): 8 x DbrxMLP(\n",
       "                (w1): Linear4bit(in_features=6144, out_features=10752, bias=False)\n",
       "                (v1): Linear4bit(in_features=6144, out_features=10752, bias=False)\n",
       "                (w2): Linear4bit(in_features=10752, out_features=6144, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=6144, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"transformers\", transformers.__version__)\n",
    "\n",
    "max_memory = {0:\"24GiB\", 1: \"24GiB\"}\n",
    "\n",
    "qConfig = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_memory = max_memory,\n",
    "    quantization_config = qConfig\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccdd395-f464-492a-8895-c72c96d132e8",
   "metadata": {},
   "source": [
    "# MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab474910-27df-4b2a-b342-6023a46b7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategories = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "# in the form to fit the prompt headline\n",
    "subcategories_en2ru = {\n",
    "    \"abstract_algebra\": \"абстрактной_алгебре\",\n",
    "    \"anatomy\": \"анатомии\",\n",
    "    \"astronomy\": \"астрономии\",\n",
    "    \"business_ethics\": \"деловой_этике\",\n",
    "    \"clinical_knowledge\": \"медицинским_знаниям\",\n",
    "    \"college_biology\": \"биологии_в_вузе\",\n",
    "    \"college_chemistry\": \"химии_в_вузе\",\n",
    "    \"college_computer_science\": \"компьютерным_наукам_в_вузе\",\n",
    "    \"college_mathematics\": \"математике_в_вузе\",\n",
    "    \"college_medicine\": \"медицине_в_вузе\",\n",
    "    \"college_physics\": \"физике_в_вузе\",\n",
    "    \"computer_security\": \"компьютерной_безопасности\",\n",
    "    \"conceptual_physics\": \"теоретической_физике\",\n",
    "    \"econometrics\": \"эконометрике\",\n",
    "    \"electrical_engineering\": \"электротехнике\",\n",
    "    \"elementary_mathematics\": \"элементарной_математике\",\n",
    "    \"formal_logic\": \"формальной_логике\",\n",
    "    \"global_facts\": \"фактам_о_мире\",\n",
    "    \"high_school_biology\": \"биологии_в_старшей_школе\",\n",
    "    \"high_school_chemistry\": \"химии_в_старшей_школе\",\n",
    "    \"high_school_computer_science\": \"информатике_в_старшей_школе\",\n",
    "    \"high_school_european_history\": \"истории_Европы_в_старшей_школе\",\n",
    "    \"high_school_geography\": \"географии_в_старшей_школе\",\n",
    "    \"high_school_government_and_politics\": \"государству_и_политике_в_старшей_школе\",\n",
    "    \"high_school_macroeconomics\": \"макроэкономике_в_старшей_школе\",\n",
    "    \"high_school_mathematics\": \"математике_в_старшей_школе\",\n",
    "    \"high_school_microeconomics\": \"микроэкономике_в_старшей_школе\",\n",
    "    \"high_school_physics\": \"физике_в_старшей_школе\",\n",
    "    \"high_school_psychology\": \"психологии_в_старшей_школе\",\n",
    "    \"high_school_statistics\": \"статистике_в_старшей_школе\",\n",
    "    \"high_school_us_history\": \"истории_США_в_старшей_школе\",\n",
    "    \"high_school_world_history\": \"всемирной_истории_в_старшей_школе\",\n",
    "    \"human_aging\": \"старению_человека\",\n",
    "    \"human_sexuality\": \"человеческой_сексуальности\",\n",
    "    \"international_law\": \"международному_праву\",\n",
    "    \"jurisprudence\": \"юриспруденции\",\n",
    "    \"logical_fallacies\": \"логическим_ошибкам\",\n",
    "    \"machine_learning\": \"машинному_обучению\",\n",
    "    \"management\": \"менеджменту\",\n",
    "    \"marketing\": \"маркетингу\",\n",
    "    \"medical_genetics\": \"медицинской_генетике\",\n",
    "    \"miscellaneous\": \"разным_темам\",\n",
    "    \"moral_disputes\": \"нравственным_спорам\",\n",
    "    \"moral_scenarios\": \"нравственным_сценариям\",\n",
    "    \"nutrition\": \"правильному_питанию\",\n",
    "    \"philosophy\": \"философии\",\n",
    "    \"prehistory\": \"доисторической_эпохе\",\n",
    "    \"professional_accounting\": \"профессиональному_бухгалтерскому_учету\",\n",
    "    \"professional_law\": \"профессиональному_праву\",\n",
    "    \"professional_medicine\": \"профессиональной_медицине\",\n",
    "    \"professional_psychology\": \"профессиональной_психологии\",\n",
    "    \"public_relations\": \"связям_с_общественностью\",\n",
    "    \"security_studies\": \"исследованиям_в_области_безопасности\",\n",
    "    \"sociology\": \"социологии\",\n",
    "    \"us_foreign_policy\": \"внешней_политике_США\",\n",
    "    \"virology\": \"вирусологии\",\n",
    "    \"world_religions\": \"мировым_религиям\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e297b065-26ff-4444-ab35-f9a5363280f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import typing as tp\n",
    "\n",
    "class Conversation(abc.ABC):\n",
    "    \"\"\"\n",
    "    Inspired by https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n",
    "    \"\"\"\n",
    "    def __init__(self, system_prompt: str = \"\", roles: tp.Tuple[str, str] = (\"user\", \"assistant\")):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.roles = roles\n",
    "        self.messages: tp.List[tp.List[str, str]] = []\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        pass\n",
    "\n",
    "    def update_last_message(self, text: str) -> None:\n",
    "        self.messages[-1] = (self.messages[-1][0], text)\n",
    "\n",
    "    def append_message(self, role: str, text: str) -> None:\n",
    "        self.messages.append({\"role\":role, \"content\":text})\n",
    "\n",
    "class EmptyConversation(Conversation):\n",
    "\n",
    "    #\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            system_prompt=\"\",\n",
    "            roles=(\"user\", \"assistant\"),\n",
    "        )\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        prompt = self.system_prompt\n",
    "        for m in self.messages:\n",
    "            prompt += str(m)\n",
    "        return prompt\n",
    "\n",
    "conversation_classes = {\n",
    "    \"empy_prompt_conv\": EmptyConversation,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e20165c-54fc-4de9-b175-feef30e8b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import typing as tp\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import peft\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LANGUAGE_CONFIG: tp.Dict[str, tp.Dict[str, str]] = {\n",
    "    \"en\": {\n",
    "        \"headline_prefix\": \"The following are multiple choice questions (with answers) about\",\n",
    "        \"answer_prefix\": \"Answer:\",\n",
    "    },\n",
    "    \"ru\": {\n",
    "        \"headline_prefix\": \"Ниже приведены вопросы с множественным выбором (с ответами) по\",\n",
    "        \"answer_prefix\": \"Ответ:\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0fc9a6-e430-4b0d-b57e-7ecbb2dd152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_in_hendrycks_format(subject: str, split: str, lang: str) -> pd.DataFrame:\n",
    "    dataset = datasets.load_dataset(\"NLPCoreTeam/mmlu_ru\", name=subject, split=split)\n",
    "    wanted_cols = {\n",
    "        \"en\": [\"question_en\", \"choices_en\", \"answer\"],\n",
    "        \"ru\": [\"question_ru\", \"choices_ru\", \"answer\"],\n",
    "    }[lang]\n",
    "    df = dataset.to_pandas()[wanted_cols]\n",
    "    int2str = dataset.features[\"answer\"].int2str\n",
    "    df[df.columns[2]] = df[df.columns[2]].apply(lambda x: int2str(x))\n",
    "    df = pd.concat([\n",
    "        df[[df.columns[0]]],\n",
    "        pd.DataFrame(df[df.columns[1]].tolist()),\n",
    "        df[[df.columns[2]]],\n",
    "    ], axis=1)\n",
    "    df.columns = range(len(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23123b1-e08b-438a-9b56-aa755978f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_subject(subject: str) -> str:\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s.strip()\n",
    "\n",
    "def get_pretty_subject(subject: str, lang: str) -> str:\n",
    "    return format_subject({\n",
    "        \"en\": subject,\n",
    "        \"ru\": subcategories_en2ru[subject],  # predefined map\n",
    "    }[lang])\n",
    "\n",
    "def get_prompt_from_dataframes(dev_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                               k: int, test_iloc_idx: int, lang: str, subject: str, conversation_type: str):\n",
    "    assert 0 <= k <= 5\n",
    "    headline_prefix = LANGUAGE_CONFIG[lang][\"headline_prefix\"]\n",
    "    headline_postfix = get_pretty_subject(subject=subject, lang=lang)\n",
    "    headline = f\"{headline_prefix} {headline_postfix}.\\n\\n\"\n",
    "\n",
    "    answer_prefix = LANGUAGE_CONFIG[lang][\"answer_prefix\"]\n",
    "\n",
    "    conv = conversation_classes[conversation_type]()\n",
    "\n",
    "    is_already_taken_headline = False\n",
    "    for row_idx, row in dev_df.head(k).iterrows():\n",
    "        q = row[0]\n",
    "        options = row[1:5].tolist()\n",
    "        lettered_options = [f\"({x}) {y}\" for x, y in zip([\"A\", \"B\", \"C\", \"D\"], options)]\n",
    "        q_with_lettered_options = \"\\n\".join([q, \" \".join(lettered_options)])\n",
    "        if row_idx == 0:\n",
    "            q_with_lettered_options = headline + q_with_lettered_options\n",
    "            is_already_taken_headline = True\n",
    "        conv.append_message(conv.roles[0], q_with_lettered_options)\n",
    "        a = row[5]\n",
    "        \n",
    "        # if is not instruct, needed to be manually separated for mmlu examples\n",
    "        conv.append_message(conv.roles[1], f\"{answer_prefix}{a}\")\n",
    "\n",
    "    row = test_df.iloc[test_iloc_idx]\n",
    "    q = row[0]\n",
    "    options = row[1:5].tolist()\n",
    "    lettered_options = [f\"({x}) {y}\" for x, y in zip([\"A\", \"B\", \"C\", \"D\"], options)]\n",
    "    q_with_lettered_options = \"\\n\".join([q, \" \".join(lettered_options)])\n",
    "    if not is_already_taken_headline:\n",
    "        q_with_lettered_options = headline + q_with_lettered_options\n",
    "        is_already_taken_headline = True\n",
    "    conv.append_message(conv.roles[0], q_with_lettered_options)\n",
    "    a = row[5]\n",
    "    conv.append_message(conv.roles[1], answer_prefix)\n",
    "    # prompt = f\"{conv.get_prompt()}{answer_prefix}\"\n",
    "    return conv.messages\n",
    "\n",
    "def calculate_token_interest_probs(\n",
    "    input_ids,\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase,\n",
    "    model: tp.Union[transformers.PreTrainedModel, peft.peft_model.PeftModelForCausalLM],\n",
    ") -> tp.Dict[str, float]:    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits  # shape (batch_size, sequence_length, vocab_size)\n",
    "    next_token_logits = logits[:, -1, :]  # shape (batch_size, vocab_size)\n",
    "\n",
    "    next_token_logits = next_token_logits.flatten()\n",
    "    assert next_token_logits.shape == torch.Size((model.config.vocab_size, ))\n",
    "\n",
    "    next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1).cpu()  # all probs over vocab\n",
    "    # assert torch.isclose(next_token_probs.sum(), torch.tensor(1.0).to(next_token_probs.dtype), atol=1e-03)  # dtype for half/nothalf, -03 for float16\n",
    "    \n",
    "    tokens_of_interest = [\n",
    "        tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "    ]\n",
    "\n",
    "    probs = next_token_probs[tokens_of_interest].tolist()\n",
    "    res = dict(zip([\"A\", \"B\", \"C\", \"D\"], probs))\n",
    "    return res\n",
    "\n",
    "def append_to_jsonl(data: list, filename: str) -> None:\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "def evaluate_subject(\n",
    "    subject: str,\n",
    "    lang: str,\n",
    "    k_shot: int,\n",
    "    jsonl_filepath: str,\n",
    "    maxlen: int,\n",
    "    convtype: str,\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase,\n",
    "    model: tp.Union[transformers.PreTrainedModel, peft.peft_model.PeftModelForCausalLM],\n",
    ") -> None:\n",
    "\n",
    "    dev_df = get_df_in_hendrycks_format(subject=subject, split=\"dev\", lang=lang)\n",
    "    test_df = get_df_in_hendrycks_format(subject=subject, split=\"test\", lang=lang)\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=subject):\n",
    "\n",
    "        current_k_shot = k_shot\n",
    "        skip_too_lengthy = False\n",
    "        while True:\n",
    "            if current_k_shot < 0:\n",
    "                logger.info(\"Skip too lengthy.\")\n",
    "                skip_too_lengthy = True\n",
    "                break\n",
    "            input_messages = get_prompt_from_dataframes(\n",
    "                dev_df=dev_df,\n",
    "                test_df=test_df,\n",
    "                k=current_k_shot,\n",
    "                test_iloc_idx=idx,\n",
    "                lang=lang,\n",
    "                subject=subject,\n",
    "                conversation_type=convtype,\n",
    "            )\n",
    "            input_prompt = tokenizer.apply_chat_template(input_messages, tokenize=False, add_special_tokens=False)[:-len(tokenizer.eos_token)]\n",
    "            input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "            if input_ids.shape[-1] > maxlen and current_k_shot >= 0:\n",
    "                logger.info(\"Takes smaller current_k_shot since maxlen.\")\n",
    "                current_k_shot -= 1\n",
    "            elif current_k_shot < 0:\n",
    "                logger.info(\"Skip too lengthy.\")\n",
    "                skip_too_lengthy = True\n",
    "            else:\n",
    "                break\n",
    "        if skip_too_lengthy:\n",
    "            continue\n",
    "\n",
    "        label = row[5]\n",
    "\n",
    "        preds = calculate_token_interest_probs(\n",
    "            input_ids=input_ids,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        append_to_jsonl(data=[input_prompt, label, preds], filename=jsonl_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7768d8-483e-4694-814a-f235a0a82c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\n",
      "YOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\n",
      "You assist with various tasks, from writing to coding (using markdown for code blocks — remember to use ``` with code, JSON, and tables).\n",
      "(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\n",
      "This is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\n",
      "YOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER'S QUERY.<|im_end|>\n",
      "<|im_start|>user\n",
      "The following are multiple choice questions (with answers) about abstract algebra.\n",
      "\n",
      "Find all c in Z_3 such that Z_3[x]/(x^2 + c) is a field.\n",
      "(A) 0 (B) 1 (C) 2 (D) 3<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Answer:B<|im_end|>\n",
      "<|im_start|>user\n",
      "Statement 1 | If aH is an element of a factor group, then |aH| divides |a|. Statement 2 | If H and K are subgroups of G then HK is a subgroup of G.\n",
      "(A) True, True (B) False, False (C) True, False (D) False, True<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Answer:B<|im_end|>\n",
      "<|im_start|>user\n",
      "Statement 1 | Every element of a group generates a cyclic subgroup of the group. Statement 2 | The symmetric group S_10 has 10 elements.\n",
      "(A) True, True (B) False, False (C) True, False (D) False, True<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Answer:C<|im_end|>\n",
      "<|im_start|>user\n",
      "Statement 1| Every function from a finite set onto itself must be one to one. Statement 2 | Every subgroup of an abelian group is abelian.\n",
      "(A) True, True (B) False, False (C) True, False (D) False, True<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Answer:A<|im_end|>\n",
      "<|im_start|>user\n",
      "Find the characteristic of the ring 2Z.\n",
      "(A) 0 (B) 3 (C) 12 (D) 30<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Answer:A<|im_end|>\n",
      "<|im_start|>user\n",
      "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "(A) 0 (B) 4 (C) 2 (D) 6<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Answer:\n",
      "CPU times: user 3.68 s, sys: 1.03 s, total: 4.71 s\n",
      "Wall time: 7.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\\nYou assist with various tasks, from writing to coding (using markdown for code blocks — remember to use ``` with code, JSON, and tables).\\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER'S QUERY. <|im_end|> \\n<|im_start|>user\\nThe following are multiple choice questions (with answers) about abstract algebra.\\n\\nFind all c in Z_3 such that Z_3[x]/(x^2 + c) is a field.\\n(A) 0 (B) 1 (C) 2 (D) 3 <|im_end|> \\n<|im_start|>assistant\\nAnswer:B <|im_end|> \\n<|im_start|>user\\nStatement 1 | If aH is an element of a factor group, then |aH| divides |a|. Statement 2 | If H and K are subgroups of G then HK is a subgroup of G.\\n(A) True, True (B) False, False (C) True, False (D) False, True <|im_end|> \\n<|im_start|>assistant\\nAnswer:B <|im_end|> \\n<|im_start|>user\\nStatement 1 | Every element of a group generates a cyclic subgroup of the group. Statement 2 | The symmetric group S_10 has 10 elements.\\n(A) True, True (B) False, False (C) True, False (D) False, True <|im_end|> \\n<|im_start|>assistant\\nAnswer:C <|im_end|> \\n<|im_start|>user\\nStatement 1| Every function from a finite set onto itself must be one to one. Statement 2 | Every subgroup of an abelian group is abelian.\\n(A) True, True (B) False, False (C) True, False (D) False, True <|im_end|> \\n<|im_start|>assistant\\nAnswer:A <|im_end|> \\n<|im_start|>user\\nFind the characteristic of the ring 2Z.\\n(A) 0 (B) 3 (C) 12 (D) 30 <|im_end|> \\n<|im_start|>assistant\\nAnswer:A <|im_end|> \\n<|im_start|>user\\nFind the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\n(A) 0 (B) 4 (C) 2 (D) 6 <|im_end|> \\n<|im_start|>assistant\\nAnswer:Q_2Z_2Z_2Z\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lang = \"en\"\n",
    "subject = \"abstract_algebra\"\n",
    "convtype = \"empy_prompt_conv\"\n",
    "current_k_shot = 5\n",
    "idx = 0\n",
    "dev_df = get_df_in_hendrycks_format(subject=subject, split=\"dev\", lang=lang)\n",
    "test_df = get_df_in_hendrycks_format(subject=subject, split=\"test\", lang=lang)\n",
    "\n",
    "input_messages = get_prompt_from_dataframes(\n",
    "    dev_df=dev_df,\n",
    "    test_df=test_df,\n",
    "    k=current_k_shot,\n",
    "    test_iloc_idx=idx,\n",
    "    lang=lang,\n",
    "    subject=subject,\n",
    "    conversation_type=convtype,\n",
    ")\n",
    "input_prompt = tokenizer.apply_chat_template(input_messages, tokenize=False, add_special_tokens=False)[:-len(tokenizer.eos_token)]\n",
    "print(input_prompt)\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=10, eos_token_id=tokenizer.eos_token_id)\n",
    "ouput_str = tokenizer.decode(output_ids[0]).strip()\n",
    "ouput_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28447643-9a5a-4161-8f7d-04a47029c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"mmlu_en_dbrx-instruct_8X_bnb4q\"\n",
    "lang = \"en\"\n",
    "k_shot = 5\n",
    "maxlen = 8192\n",
    "convtype = \"empy_prompt_conv\"\n",
    "\n",
    "subjects = list(subcategories.keys())\n",
    "for each_subject in subjects:\n",
    "    jsonl_filepath = str(pathlib.Path(output_dir) / f\"{each_subject}.jsonl\")\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Filepath JSONL: {jsonl_filepath}\")\n",
    "    if pathlib.Path(jsonl_filepath).exists():\n",
    "        logger.info(f\"File already exists! Please manually verify that it wasn't partially interrupted.\")\n",
    "        continue\n",
    "    evaluate_subject(\n",
    "            subject=each_subject,\n",
    "            lang=lang,\n",
    "            k_shot=k_shot,\n",
    "            jsonl_filepath=jsonl_filepath,\n",
    "            maxlen=maxlen, convtype=convtype,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f3aaffe-ecf3-499b-9742-77741d8cc55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmlu_en_dbrx-instruct_8X_bnb4q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mmlu_en_dbrx-instruct_8X_bnb4q\n",
       "0                       24.333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "category_to_main_category = {value: key for key, sublist in categories.items() for value in sublist}\n",
    "subcategories2categories = {key: category_to_main_category[value[0]] for key, value in subcategories.items()}\n",
    "\n",
    "def calculate_accuracy_from_directory(dirpath: str) -> tp.Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    assert pathlib.Path(dirpath).exists()\n",
    "    filepaths = [str(x) for x in pathlib.Path(dirpath).glob('*.jsonl')]\n",
    "    # assert len(filepaths) == 57\n",
    "    res = {}\n",
    "    for each_filepath in filepaths:\n",
    "        df = pd.read_json(each_filepath, lines=True)\n",
    "        df.columns = ['prompt', 'label', 'preds']\n",
    "        cors = []\n",
    "        for idx, row in df.iterrows():\n",
    "            preds = row['preds']\n",
    "            best_idx = np.argmax(list(preds.values()))\n",
    "            y_pred = list(preds.keys())[best_idx]\n",
    "            y_true = row['label']\n",
    "            y_pred = y_pred.strip()\n",
    "            y_true = y_true.strip()\n",
    "            cors.append(y_true == y_pred)\n",
    "        acc = np.mean(cors)\n",
    "        res[pathlib.Path(each_filepath).stem] = acc * 100\n",
    "    \n",
    "    df = pd.DataFrame({pathlib.Path(dirpath).stem: res}).reset_index()\n",
    "    df = df.rename(columns={'index': 'subcategory'})\n",
    "    subcategories_df = df.copy()\n",
    "    \n",
    "    df = subcategories_df.copy()\n",
    "    df['subcategory'] = df['subcategory'].map(subcategories2categories)\n",
    "    df = df.rename(columns={'subcategory': 'category'})\n",
    "    df = df.groupby('category').mean().reset_index()\n",
    "    categories_df = df.copy()\n",
    "    \n",
    "    total_df = pd.DataFrame({pathlib.Path(dirpath).stem: [categories_df[pathlib.Path(dirpath).stem].mean()]})\n",
    "    \n",
    "    # assert subcategories_df.shape == (57, 2)\n",
    "    # assert categories_df.shape == (4, 2)\n",
    "    # assert total_df.shape == (1, 1)\n",
    "    return (subcategories_df, categories_df, total_df)\n",
    "\n",
    "subcategories_df, categories_df, total_df = calculate_accuracy_from_directory(dirpath=output_dir)\n",
    "print(total_df.shape)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c881423-f769-474f-b931-651a9afd8ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>mmlu_en_dbrx-instruct_8X_bnb4q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STEM</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other (business, health, misc.)</td>\n",
       "      <td>26.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category  mmlu_en_dbrx-instruct_8X_bnb4q\n",
       "0                             STEM                       22.000000\n",
       "1  other (business, health, misc.)                       26.666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ec58653-0ad0-4f17-b179-9db4b586f462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>mmlu_en_dbrx-instruct_8X_bnb4q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>26.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subcategory  mmlu_en_dbrx-instruct_8X_bnb4q\n",
       "0  abstract_algebra                       22.000000\n",
       "1           anatomy                       26.666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subcategories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70b63c-8265-4fb2-bc1f-699046915d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
