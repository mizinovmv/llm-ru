{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78723c7d-adeb-4f2c-be0a-915b0c702667",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategories = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "# in the form to fit the prompt headline\n",
    "subcategories_en2ru = {\n",
    "    \"abstract_algebra\": \"абстрактной_алгебре\",\n",
    "    \"anatomy\": \"анатомии\",\n",
    "    \"astronomy\": \"астрономии\",\n",
    "    \"business_ethics\": \"деловой_этике\",\n",
    "    \"clinical_knowledge\": \"медицинским_знаниям\",\n",
    "    \"college_biology\": \"биологии_в_вузе\",\n",
    "    \"college_chemistry\": \"химии_в_вузе\",\n",
    "    \"college_computer_science\": \"компьютерным_наукам_в_вузе\",\n",
    "    \"college_mathematics\": \"математике_в_вузе\",\n",
    "    \"college_medicine\": \"медицине_в_вузе\",\n",
    "    \"college_physics\": \"физике_в_вузе\",\n",
    "    \"computer_security\": \"компьютерной_безопасности\",\n",
    "    \"conceptual_physics\": \"теоретической_физике\",\n",
    "    \"econometrics\": \"эконометрике\",\n",
    "    \"electrical_engineering\": \"электротехнике\",\n",
    "    \"elementary_mathematics\": \"элементарной_математике\",\n",
    "    \"formal_logic\": \"формальной_логике\",\n",
    "    \"global_facts\": \"фактам_о_мире\",\n",
    "    \"high_school_biology\": \"биологии_в_старшей_школе\",\n",
    "    \"high_school_chemistry\": \"химии_в_старшей_школе\",\n",
    "    \"high_school_computer_science\": \"информатике_в_старшей_школе\",\n",
    "    \"high_school_european_history\": \"истории_Европы_в_старшей_школе\",\n",
    "    \"high_school_geography\": \"географии_в_старшей_школе\",\n",
    "    \"high_school_government_and_politics\": \"государству_и_политике_в_старшей_школе\",\n",
    "    \"high_school_macroeconomics\": \"макроэкономике_в_старшей_школе\",\n",
    "    \"high_school_mathematics\": \"математике_в_старшей_школе\",\n",
    "    \"high_school_microeconomics\": \"микроэкономике_в_старшей_школе\",\n",
    "    \"high_school_physics\": \"физике_в_старшей_школе\",\n",
    "    \"high_school_psychology\": \"психологии_в_старшей_школе\",\n",
    "    \"high_school_statistics\": \"статистике_в_старшей_школе\",\n",
    "    \"high_school_us_history\": \"истории_США_в_старшей_школе\",\n",
    "    \"high_school_world_history\": \"всемирной_истории_в_старшей_школе\",\n",
    "    \"human_aging\": \"старению_человека\",\n",
    "    \"human_sexuality\": \"человеческой_сексуальности\",\n",
    "    \"international_law\": \"международному_праву\",\n",
    "    \"jurisprudence\": \"юриспруденции\",\n",
    "    \"logical_fallacies\": \"логическим_ошибкам\",\n",
    "    \"machine_learning\": \"машинному_обучению\",\n",
    "    \"management\": \"менеджменту\",\n",
    "    \"marketing\": \"маркетингу\",\n",
    "    \"medical_genetics\": \"медицинской_генетике\",\n",
    "    \"miscellaneous\": \"разным_темам\",\n",
    "    \"moral_disputes\": \"нравственным_спорам\",\n",
    "    \"moral_scenarios\": \"нравственным_сценариям\",\n",
    "    \"nutrition\": \"правильному_питанию\",\n",
    "    \"philosophy\": \"философии\",\n",
    "    \"prehistory\": \"доисторической_эпохе\",\n",
    "    \"professional_accounting\": \"профессиональному_бухгалтерскому_учету\",\n",
    "    \"professional_law\": \"профессиональному_праву\",\n",
    "    \"professional_medicine\": \"профессиональной_медицине\",\n",
    "    \"professional_psychology\": \"профессиональной_психологии\",\n",
    "    \"public_relations\": \"связям_с_общественностью\",\n",
    "    \"security_studies\": \"исследованиям_в_области_безопасности\",\n",
    "    \"sociology\": \"социологии\",\n",
    "    \"us_foreign_policy\": \"внешней_политике_США\",\n",
    "    \"virology\": \"вирусологии\",\n",
    "    \"world_religions\": \"мировым_религиям\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48491ed-f0c6-4250-a422-ebec70877624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import typing as tp\n",
    "\n",
    "class Conversation(abc.ABC):\n",
    "    \"\"\"\n",
    "    Inspired by https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n",
    "    \"\"\"\n",
    "    def __init__(self, system_prompt: str, roles: tp.Tuple[str, str]):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.roles = roles\n",
    "        self.messages: tp.List[tp.Tuple[str, str]] = []\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_prompt(self) -> str:\n",
    "        pass\n",
    "\n",
    "    def update_last_message(self, text: str) -> None:\n",
    "        self.messages[-1] = (self.messages[-1][0], text)\n",
    "\n",
    "    def append_message(self, role: str, text: str) -> None:\n",
    "        self.messages.append([role, text])\n",
    "\n",
    "class EmptyConversation(Conversation):\n",
    "\n",
    "    #\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            system_prompt=\"\",\n",
    "            roles=(\"\", \"\"),\n",
    "        )\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        prompt = self.system_prompt\n",
    "        for role, text in self.messages:\n",
    "            if text:\n",
    "                prompt += f\"{role}{text}\"\n",
    "            else:\n",
    "                prompt += f\"{role}\"\n",
    "        return prompt\n",
    "\n",
    "conversation_classes = {\n",
    "    \"empy_prompt_conv\": EmptyConversation,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc89932c-5691-4a3c-b809-5ee3e5788d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-20 21:29:27,820] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import typing as tp\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import peft\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LANGUAGE_CONFIG: tp.Dict[str, tp.Dict[str, str]] = {\n",
    "    \"en\": {\n",
    "        \"headline_prefix\": \"The following are multiple choice questions (with answers) about\",\n",
    "        \"answer_prefix\": \"Answer:\",\n",
    "    },\n",
    "    \"ru\": {\n",
    "        \"headline_prefix\": \"Ниже приведены вопросы с множественным выбором (с ответами) по\",\n",
    "        \"answer_prefix\": \"Ответ:\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf49c5db-1a0a-4972-969b-fdaace80bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_in_hendrycks_format(subject: str, split: str, lang: str) -> pd.DataFrame:\n",
    "    dataset = datasets.load_dataset(\"NLPCoreTeam/mmlu_ru\", name=subject, split=split, use_auth_token=True)\n",
    "    wanted_cols = {\n",
    "        \"en\": [\"question_en\", \"choices_en\", \"answer\"],\n",
    "        \"ru\": [\"question_ru\", \"choices_ru\", \"answer\"],\n",
    "    }[lang]\n",
    "    df = dataset.to_pandas()[wanted_cols]\n",
    "    int2str = dataset.features[\"answer\"].int2str\n",
    "    df[df.columns[2]] = df[df.columns[2]].apply(lambda x: int2str(x))\n",
    "    df = pd.concat([\n",
    "        df[[df.columns[0]]],\n",
    "        pd.DataFrame(df[df.columns[1]].tolist()),\n",
    "        df[[df.columns[2]]],\n",
    "    ], axis=1)\n",
    "    df.columns = range(len(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72bdba4-05e7-4fd4-90d0-038b7ce1a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-1_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259d895a-c5af-4031-994d-ceccafb10425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.0.1+cu118\n",
      "transformers 4.33.2\n",
      "[2023-09-20 21:37:11,748] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixFormerSequentialForCausalLM(\n",
       "  (layers): Sequential(\n",
       "    (0): Embedding(\n",
       "      (wte): Embedding(51200, 2048)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (2): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (3): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (4): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (5): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (6): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (7): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (8): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (9): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (10): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (11): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (12): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (13): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (14): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (15): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (16): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (17): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (18): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (19): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (20): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (21): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (22): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (23): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (24): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (25): CausalLMHead(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CausalLMLoss(\n",
       "    (loss_fct): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"transformers\", transformers.__version__)\n",
    "\n",
    "device_map = \"auto\"\n",
    "max_memory = None\n",
    "max_memory = {0:\"16GiB\"}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map,\n",
    "    max_memory=max_memory,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1673878a-3eb4-4421-a7f5-f21787faed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenTokenizer(name_or_path='microsoft/phi-1_5', vocab_size=50257, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81eff76c-d0ac-4f92-81e6-1ebce44e822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_subject(subject: str) -> str:\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s.strip()\n",
    "\n",
    "def get_pretty_subject(subject: str, lang: str) -> str:\n",
    "    return format_subject({\n",
    "        \"en\": subject,\n",
    "        \"ru\": subcategories_en2ru[subject],  # predefined map\n",
    "    }[lang])\n",
    "\n",
    "def get_prompt_from_dataframes(dev_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                               k: int, test_iloc_idx: int, lang: str, subject: str, conversation_type: str):\n",
    "    assert 0 <= k <= 5\n",
    "    headline_prefix = LANGUAGE_CONFIG[lang][\"headline_prefix\"]\n",
    "    headline_postfix = get_pretty_subject(subject=subject, lang=lang)\n",
    "    headline = f\"{headline_prefix} {headline_postfix}.\\n\\n\"\n",
    "\n",
    "    answer_prefix = LANGUAGE_CONFIG[lang][\"answer_prefix\"]\n",
    "\n",
    "    conv = conversation_classes[conversation_type]()\n",
    "\n",
    "    is_already_taken_headline = False\n",
    "    for row_idx, row in dev_df.head(k).iterrows():\n",
    "        q = row[0]\n",
    "        options = row[1:5].tolist()\n",
    "        lettered_options = [f\"{x}. {y}\" for x, y in zip([\"A\", \"B\", \"C\", \"D\"], options)]\n",
    "        q_with_lettered_options = \"\\n\".join([q] + lettered_options)\n",
    "        if row_idx == 0:\n",
    "            q_with_lettered_options = headline + q_with_lettered_options\n",
    "            is_already_taken_headline = True\n",
    "        conv.append_message(conv.roles[0], q_with_lettered_options)\n",
    "        a = row[5]\n",
    "        \n",
    "        # if is not instruct, needed to be manually separated for mmlu examples\n",
    "        if conv.roles == (\"\", \"\"):\n",
    "            conv.append_message(conv.roles[1], f\"\\n{answer_prefix}{a}\\n\\n\")\n",
    "        else:\n",
    "            conv.append_message(conv.roles[1], f\"\\n{answer_prefix}{a}\")\n",
    "\n",
    "    row = test_df.iloc[test_iloc_idx]\n",
    "    q = row[0]\n",
    "    options = row[1:5].tolist()\n",
    "    lettered_options = [f\"{x}. {y}\" for x, y in zip([\"A\", \"B\", \"C\", \"D\"], options)]\n",
    "    q_with_lettered_options = \"\\n\".join([q] + lettered_options)\n",
    "    if not is_already_taken_headline:\n",
    "        q_with_lettered_options = headline + q_with_lettered_options\n",
    "        is_already_taken_headline = True\n",
    "    conv.append_message(conv.roles[0], q_with_lettered_options)\n",
    "    a = row[5]\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    \n",
    "    prompt = f\"{conv.get_prompt()}\\n{answer_prefix}\"\n",
    "    return prompt\n",
    "\n",
    "def calculate_token_interest_probs(\n",
    "    input_prompt: str,\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase,\n",
    "    model: tp.Union[transformers.PreTrainedModel, peft.peft_model.PeftModelForCausalLM],\n",
    ") -> tp.Dict[str, float]:\n",
    "    assert isinstance(input_prompt, str)\n",
    "    input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits  # shape (batch_size, sequence_length, vocab_size)\n",
    "    next_token_logits = logits[:, -1, :]  # shape (batch_size, vocab_size)\n",
    "\n",
    "    next_token_logits = next_token_logits.flatten()\n",
    "    assert next_token_logits.shape == torch.Size((model.config.vocab_size, ))\n",
    "\n",
    "    next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1).cpu()  # all probs over vocab\n",
    "    # assert torch.isclose(next_token_probs.sum(), torch.tensor(1.0).to(next_token_probs.dtype), atol=1e-03)  # dtype for half/nothalf, -03 for float16\n",
    "    \n",
    "    tokens_of_interest = [\n",
    "        tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "    ]\n",
    "    \n",
    "    probs = next_token_probs[tokens_of_interest].tolist()\n",
    "    res = dict(zip([\"A\", \"B\", \"C\", \"D\"], probs))\n",
    "    return res\n",
    "\n",
    "def append_to_jsonl(data: list, filename: str) -> None:\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "def evaluate_subject(\n",
    "    subject: str,\n",
    "    lang: str,\n",
    "    k_shot: int,\n",
    "    jsonl_filepath: str,\n",
    "    maxlen: int,\n",
    "    convtype: str,\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase,\n",
    "    model: tp.Union[transformers.PreTrainedModel, peft.peft_model.PeftModelForCausalLM],\n",
    ") -> None:\n",
    "\n",
    "    dev_df = get_df_in_hendrycks_format(subject=subject, split=\"dev\", lang=lang)\n",
    "    test_df = get_df_in_hendrycks_format(subject=subject, split=\"test\", lang=lang)\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=subject):\n",
    "\n",
    "        current_k_shot = k_shot\n",
    "        skip_too_lengthy = False\n",
    "        while True:\n",
    "            if current_k_shot < 0:\n",
    "                logger.info(\"Skip too lengthy.\")\n",
    "                skip_too_lengthy = True\n",
    "                break\n",
    "            input_prompt = get_prompt_from_dataframes(\n",
    "                dev_df=dev_df,\n",
    "                test_df=test_df,\n",
    "                k=current_k_shot,\n",
    "                test_iloc_idx=idx,\n",
    "                lang=lang,\n",
    "                subject=subject,\n",
    "                conversation_type=convtype,\n",
    "            )\n",
    "            input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "            if input_ids.shape[-1] > maxlen and current_k_shot >= 0:\n",
    "                logger.info(\"Takes smaller current_k_shot since maxlen.\")\n",
    "                current_k_shot -= 1\n",
    "            elif current_k_shot < 0:\n",
    "                logger.info(\"Skip too lengthy.\")\n",
    "                skip_too_lengthy = True\n",
    "            else:\n",
    "                break\n",
    "        if skip_too_lengthy:\n",
    "            continue\n",
    "\n",
    "        label = row[5]\n",
    "\n",
    "        preds = calculate_token_interest_probs(\n",
    "            input_prompt=input_prompt,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        append_to_jsonl(data=[input_prompt, label, preds], filename=jsonl_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3935963-d5d7-4b60-b69c-08436870a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset mmlu_ru (/home/mmv/.cache/huggingface/datasets/NLPCoreTeam___mmlu_ru/abstract_algebra/1.0.0/5f8c605caf405b545e54fa796577e79a3fa9fc757b9842bd4eb911133dcf5b8b)\n",
      "WARNING:datasets.builder:Found cached dataset mmlu_ru (/home/mmv/.cache/huggingface/datasets/NLPCoreTeam___mmlu_ru/abstract_algebra/1.0.0/5f8c605caf405b545e54fa796577e79a3fa9fc757b9842bd4eb911133dcf5b8b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ниже приведены вопросы с множественным выбором (с ответами) по абстрактной алгебре.\n",
      "\n",
      "Найдите все c в Z_3 таким образом, чтобы Z_3[x]/(x ^ 2 + c) было полем.\n",
      "A. 0\n",
      "B. 1\n",
      "C. 2\n",
      "D. 3\n",
      "Ответ:B\n",
      "\n",
      "Утверждение 1 | Если aH является элементом факторной группы, то |aH| делит |a|. Утверждение 2 | Если H и K являются подгруппами G, то HK является подгруппой G.\n",
      "A. Верно, верно\n",
      "B. Ложь, ложь\n",
      "C. Правда, ложь\n",
      "D. Ложь, истина\n",
      "Ответ:B\n",
      "\n",
      "Утверждение 1 | Каждый элемент группы порождает циклическую подгруппу группы. Утверждение 2 | Симметричная группа S_10 состоит из 10 элементов.\n",
      "A. Верно, верно\n",
      "B. Ложь, ложь\n",
      "C. Правда, ложь\n",
      "D. Ложь, истина\n",
      "Ответ:C\n",
      "\n",
      "Утверждение 1| Каждая функция из конечного множества на саму себя должна быть один к одному. Утверждение 2 | Каждая подгруппа абелевой группы является абелевой.\n",
      "A. Верно, верно\n",
      "B. Ложь, ложь\n",
      "C. Правда, ложь\n",
      "D. Ложь, истина\n",
      "Ответ:A\n",
      "\n",
      "Найдите характеристику кольца 2Z.\n",
      "A. 0\n",
      "B. 3\n",
      "C. 12\n",
      "D. 30\n",
      "Ответ:A\n",
      "\n",
      "Найдите степень для данного расширения поля Q(sqrt(2), sqrt(3), sqrt(18)) над Q.\n",
      "A. 0\n",
      "B. 4\n",
      "C. 2\n",
      "D. 6\n",
      "Ответ:\n",
      "CPU times: user 2.21 s, sys: 42.5 ms, total: 2.25 s\n",
      "Wall time: 5.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ниже приведены вопросы с множественным выбором (с ответами) по абстрактной алгебре.\\n\\nНайдите все c в Z_3 таким образом, чтобы Z_3[x]/(x ^ 2 + c) было полем.\\nA. 0\\nB. 1\\nC. 2\\nD. 3\\nОтвет:B\\n\\nУтверждение 1 | Если aH является элементом факторной группы, то |aH| делит |a|. Утверждение 2 | Если H и K являются подгруппами G, то HK является подгруппой G.\\nA. Верно, верно\\nB. Ложь, ложь\\nC. Правда, ложь\\nD. Ложь, истина\\nОтвет:B\\n\\nУтверждение 1 | Каждый элемент группы порождает циклическую подгруппу группы. Утверждение 2 | Симметричная группа S_10 состоит из 10 элементов.\\nA. Верно, верно\\nB. Ложь, ложь\\nC. Правда, ложь\\nD. Ложь, истина\\nОтвет:C\\n\\nУтверждение 1| Каждая функция из конечного множества на саму себя должна быть один к одному. Утверждение 2 | Каждая подгруппа абелевой группы является абелевой.\\nA. Верно, верно\\nB. Ложь, ложь\\nC. Правда, ложь\\nD. Ложь, истина\\nОтвет:A\\n\\nНайдите характеристику кольца 2Z.\\nA. 0\\nB. 3\\nC. 12\\nD. 30\\nОтвет:A\\n\\nНайдите степень для данного расширения поля Q(sqrt(2), sqrt(3), sqrt(18)) над Q.\\nA. 0\\nB. 4\\nC. 2\\nD. 6\\nОтвет:B\\n\\nНайди'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lang = \"ru\"\n",
    "subject = \"abstract_algebra\"\n",
    "convtype = \"empy_prompt_conv\"\n",
    "current_k_shot = 5\n",
    "idx = 0\n",
    "dev_df = get_df_in_hendrycks_format(subject=subject, split=\"dev\", lang=lang)\n",
    "test_df = get_df_in_hendrycks_format(subject=subject, split=\"test\", lang=lang)\n",
    "input_prompt = get_prompt_from_dataframes(\n",
    "    dev_df=dev_df,\n",
    "                test_df=test_df,\n",
    "                k=current_k_shot,\n",
    "                test_iloc_idx=idx,\n",
    "                lang=lang,\n",
    "                subject=subject,\n",
    "                conversation_type=convtype,\n",
    "            )\n",
    "print(input_prompt)\n",
    "input_ids = tokenizer(\n",
    "    input_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids=False,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    ")['input_ids'].to(model.device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=10)\n",
    "ouput_str = tokenizer.decode(output_ids[0])\n",
    "ouput_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e07f7-1edf-4ecf-a86c-ecee3959fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"mmlu_ru_phi-1_5\"\n",
    "lang = \"ru\"\n",
    "k_shot = 5\n",
    "maxlen = 2048\n",
    "convtype = \"empy_prompt_conv\"\n",
    "\n",
    "subjects = list(subcategories.keys())\n",
    "for each_subject in subjects:\n",
    "    jsonl_filepath = str(pathlib.Path(output_dir) / f\"{each_subject}.jsonl\")\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Filepath JSONL: {jsonl_filepath}\")\n",
    "    if pathlib.Path(jsonl_filepath).exists():\n",
    "        logger.info(f\"File already exists! Please manually verify that it wasn't partially interrupted.\")\n",
    "        continue\n",
    "    evaluate_subject(\n",
    "            subject=each_subject,\n",
    "            lang=lang,\n",
    "            k_shot=k_shot,\n",
    "            jsonl_filepath=jsonl_filepath,\n",
    "            maxlen=maxlen, convtype=convtype,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cafd3203-0f79-434e-8466-7e67c9c4b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"mmlu_ru_phi-1_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0ae2ce-8cce-4b80-a940-70caa34f3d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmlu_ru_phi-1_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.657924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mmlu_ru_phi-1_5\n",
       "0        24.657924"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "category_to_main_category = {value: key for key, sublist in categories.items() for value in sublist}\n",
    "subcategories2categories = {key: category_to_main_category[value[0]] for key, value in subcategories.items()}\n",
    "\n",
    "def calculate_accuracy_from_directory(dirpath: str) -> tp.Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    assert pathlib.Path(dirpath).exists()\n",
    "    filepaths = [str(x) for x in pathlib.Path(dirpath).glob('*.jsonl')]\n",
    "    # assert len(filepaths) == 57\n",
    "    res = {}\n",
    "    for each_filepath in filepaths:\n",
    "        df = pd.read_json(each_filepath, lines=True)\n",
    "        df.columns = ['prompt', 'label', 'preds']\n",
    "        cors = []\n",
    "        for idx, row in df.iterrows():\n",
    "            preds = row['preds']\n",
    "            best_idx = np.argmax(list(preds.values()))\n",
    "            y_pred = list(preds.keys())[best_idx]\n",
    "            y_true = row['label']\n",
    "            y_pred = y_pred.strip()\n",
    "            y_true = y_true.strip()\n",
    "            cors.append(y_true == y_pred)\n",
    "        acc = np.mean(cors)\n",
    "        res[pathlib.Path(each_filepath).stem] = acc * 100\n",
    "    \n",
    "    df = pd.DataFrame({pathlib.Path(dirpath).stem: res}).reset_index()\n",
    "    df = df.rename(columns={'index': 'subcategory'})\n",
    "    subcategories_df = df.copy()\n",
    "    \n",
    "    df = subcategories_df.copy()\n",
    "    df['subcategory'] = df['subcategory'].map(subcategories2categories)\n",
    "    df = df.rename(columns={'subcategory': 'category'})\n",
    "    df = df.groupby('category').mean().reset_index()\n",
    "    categories_df = df.copy()\n",
    "    \n",
    "    total_df = pd.DataFrame({pathlib.Path(dirpath).stem: [categories_df[pathlib.Path(dirpath).stem].mean()]})\n",
    "    \n",
    "    # assert subcategories_df.shape == (57, 2)\n",
    "    # assert categories_df.shape == (4, 2)\n",
    "    # assert total_df.shape == (1, 1)\n",
    "    return (subcategories_df, categories_df, total_df)\n",
    "\n",
    "subcategories_df, categories_df, total_df = calculate_accuracy_from_directory(dirpath=output_dir)\n",
    "print(total_df.shape)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea70f8f-336c-4b32-98c0-5c0b81210a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>mmlu_ru_phi-1_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STEM</td>\n",
       "      <td>24.575920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>humanities</td>\n",
       "      <td>22.288360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other (business, health, misc.)</td>\n",
       "      <td>26.328821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social sciences</td>\n",
       "      <td>25.438596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category  mmlu_ru_phi-1_5\n",
       "0                             STEM        24.575920\n",
       "1                       humanities        22.288360\n",
       "2  other (business, health, misc.)        26.328821\n",
       "3                  social sciences        25.438596"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c61a104-ac1e-4925-ab32-6221d3124e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>mmlu_ru_phi-1_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>21.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>astronomy</td>\n",
       "      <td>21.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business_ethics</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>27.924528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>college_biology</td>\n",
       "      <td>27.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>college_chemistry</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>college_computer_science</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>college_mathematics</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>college_medicine</td>\n",
       "      <td>20.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>college_physics</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>computer_security</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>conceptual_physics</td>\n",
       "      <td>31.914894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>econometrics</td>\n",
       "      <td>25.438596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>electrical_engineering</td>\n",
       "      <td>23.448276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>elementary_mathematics</td>\n",
       "      <td>23.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>formal_logic</td>\n",
       "      <td>21.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>global_facts</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>high_school_biology</td>\n",
       "      <td>24.516129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>high_school_chemistry</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>high_school_computer_science</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>high_school_european_history</td>\n",
       "      <td>23.148148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     subcategory  mmlu_ru_phi-1_5\n",
       "0               abstract_algebra        24.000000\n",
       "1                        anatomy        21.481481\n",
       "2                      astronomy        21.052632\n",
       "3                business_ethics        30.000000\n",
       "4             clinical_knowledge        27.924528\n",
       "5                college_biology        27.083333\n",
       "6              college_chemistry        27.000000\n",
       "7       college_computer_science        17.000000\n",
       "8            college_mathematics        27.000000\n",
       "9               college_medicine        20.238095\n",
       "10               college_physics        16.666667\n",
       "11             computer_security        27.000000\n",
       "12            conceptual_physics        31.914894\n",
       "13                  econometrics        25.438596\n",
       "14        electrical_engineering        23.448276\n",
       "15        elementary_mathematics        23.809524\n",
       "16                  formal_logic        21.428571\n",
       "17                  global_facts        32.000000\n",
       "18           high_school_biology        24.516129\n",
       "19         high_school_chemistry        28.571429\n",
       "20  high_school_computer_science        25.000000\n",
       "21  high_school_european_history        23.148148"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subcategories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f517bb-a706-4baf-8e51-bc17a70af3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaea0ec-c3b1-4748-8cea-8bc6a611642c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
