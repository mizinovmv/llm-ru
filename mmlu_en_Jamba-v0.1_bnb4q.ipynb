{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5146af-3d1a-4922-8906-cda830e5894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78723c7d-adeb-4f2c-be0a-915b0c702667",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategories = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "# in the form to fit the prompt headline\n",
    "subcategories_en2ru = {\n",
    "    \"abstract_algebra\": \"абстрактной_алгебре\",\n",
    "    \"anatomy\": \"анатомии\",\n",
    "    \"astronomy\": \"астрономии\",\n",
    "    \"business_ethics\": \"деловой_этике\",\n",
    "    \"clinical_knowledge\": \"медицинским_знаниям\",\n",
    "    \"college_biology\": \"биологии_в_вузе\",\n",
    "    \"college_chemistry\": \"химии_в_вузе\",\n",
    "    \"college_computer_science\": \"компьютерным_наукам_в_вузе\",\n",
    "    \"college_mathematics\": \"математике_в_вузе\",\n",
    "    \"college_medicine\": \"медицине_в_вузе\",\n",
    "    \"college_physics\": \"физике_в_вузе\",\n",
    "    \"computer_security\": \"компьютерной_безопасности\",\n",
    "    \"conceptual_physics\": \"теоретической_физике\",\n",
    "    \"econometrics\": \"эконометрике\",\n",
    "    \"electrical_engineering\": \"электротехнике\",\n",
    "    \"elementary_mathematics\": \"элементарной_математике\",\n",
    "    \"formal_logic\": \"формальной_логике\",\n",
    "    \"global_facts\": \"фактам_о_мире\",\n",
    "    \"high_school_biology\": \"биологии_в_старшей_школе\",\n",
    "    \"high_school_chemistry\": \"химии_в_старшей_школе\",\n",
    "    \"high_school_computer_science\": \"информатике_в_старшей_школе\",\n",
    "    \"high_school_european_history\": \"истории_Европы_в_старшей_школе\",\n",
    "    \"high_school_geography\": \"географии_в_старшей_школе\",\n",
    "    \"high_school_government_and_politics\": \"государству_и_политике_в_старшей_школе\",\n",
    "    \"high_school_macroeconomics\": \"макроэкономике_в_старшей_школе\",\n",
    "    \"high_school_mathematics\": \"математике_в_старшей_школе\",\n",
    "    \"high_school_microeconomics\": \"микроэкономике_в_старшей_школе\",\n",
    "    \"high_school_physics\": \"физике_в_старшей_школе\",\n",
    "    \"high_school_psychology\": \"психологии_в_старшей_школе\",\n",
    "    \"high_school_statistics\": \"статистике_в_старшей_школе\",\n",
    "    \"high_school_us_history\": \"истории_США_в_старшей_школе\",\n",
    "    \"high_school_world_history\": \"всемирной_истории_в_старшей_школе\",\n",
    "    \"human_aging\": \"старению_человека\",\n",
    "    \"human_sexuality\": \"человеческой_сексуальности\",\n",
    "    \"international_law\": \"международному_праву\",\n",
    "    \"jurisprudence\": \"юриспруденции\",\n",
    "    \"logical_fallacies\": \"логическим_ошибкам\",\n",
    "    \"machine_learning\": \"машинному_обучению\",\n",
    "    \"management\": \"менеджменту\",\n",
    "    \"marketing\": \"маркетингу\",\n",
    "    \"medical_genetics\": \"медицинской_генетике\",\n",
    "    \"miscellaneous\": \"разным_темам\",\n",
    "    \"moral_disputes\": \"нравственным_спорам\",\n",
    "    \"moral_scenarios\": \"нравственным_сценариям\",\n",
    "    \"nutrition\": \"правильному_питанию\",\n",
    "    \"philosophy\": \"философии\",\n",
    "    \"prehistory\": \"доисторической_эпохе\",\n",
    "    \"professional_accounting\": \"профессиональному_бухгалтерскому_учету\",\n",
    "    \"professional_law\": \"профессиональному_праву\",\n",
    "    \"professional_medicine\": \"профессиональной_медицине\",\n",
    "    \"professional_psychology\": \"профессиональной_психологии\",\n",
    "    \"public_relations\": \"связям_с_общественностью\",\n",
    "    \"security_studies\": \"исследованиям_в_области_безопасности\",\n",
    "    \"sociology\": \"социологии\",\n",
    "    \"us_foreign_policy\": \"внешней_политике_США\",\n",
    "    \"virology\": \"вирусологии\",\n",
    "    \"world_religions\": \"мировым_религиям\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48491ed-f0c6-4250-a422-ebec70877624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import typing as tp\n",
    "\n",
    "class Conversation(abc.ABC):\n",
    "    \"\"\"\n",
    "    Inspired by https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n",
    "    \"\"\"\n",
    "    def __init__(self, system_prompt: str, roles: tp.Tuple[str, str]):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.roles = roles\n",
    "        self.messages: tp.List[tp.List[str, str]] = []\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_prompt(self) -> str:\n",
    "        pass\n",
    "\n",
    "    def update_last_message(self, text: str) -> None:\n",
    "        self.messages[-1] = (self.messages[-1][0], text)\n",
    "\n",
    "    def append_message(self, role: str, text: str) -> None:\n",
    "        self.messages.append({\"role\":role, \"content\":text})\n",
    "\n",
    "class EmptyConversation(Conversation):\n",
    "\n",
    "    #\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            system_prompt=\"\",\n",
    "            roles=(\"user\", \"assistant\"),\n",
    "        )\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        prompt = self.system_prompt\n",
    "        for m in self.messages:\n",
    "            prompt += str(m)\n",
    "        return prompt\n",
    "\n",
    "conversation_classes = {\n",
    "    \"empy_prompt_conv\": EmptyConversation,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc89932c-5691-4a3c-b809-5ee3e5788d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import typing as tp\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import peft\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LANGUAGE_CONFIG: tp.Dict[str, tp.Dict[str, str]] = {\n",
    "    \"en\": {\n",
    "        \"headline_prefix\": \"The following are multiple choice questions (with answers) about\",\n",
    "        \"answer_prefix\": \"Answer:\",\n",
    "    },\n",
    "    \"ru\": {\n",
    "        \"headline_prefix\": \"Ниже приведены вопросы с множественным выбором (с ответами) по\",\n",
    "        \"answer_prefix\": \"Ответ:\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf49c5db-1a0a-4972-969b-fdaace80bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_in_hendrycks_format(subject: str, split: str, lang: str) -> pd.DataFrame:\n",
    "    dataset = datasets.load_dataset(\"NLPCoreTeam/mmlu_ru\", name=subject, split=split)\n",
    "    wanted_cols = {\n",
    "        \"en\": [\"question_en\", \"choices_en\", \"answer\"],\n",
    "        \"ru\": [\"question_ru\", \"choices_ru\", \"answer\"],\n",
    "    }[lang]\n",
    "    df = dataset.to_pandas()[wanted_cols]\n",
    "    int2str = dataset.features[\"answer\"].int2str\n",
    "    df[df.columns[2]] = df[df.columns[2]].apply(lambda x: int2str(x))\n",
    "    df = pd.concat([\n",
    "        df[[df.columns[0]]],\n",
    "        pd.DataFrame(df[df.columns[1]].tolist()),\n",
    "        df[[df.columns[2]]],\n",
    "    ], axis=1)\n",
    "    df.columns = range(len(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72bdba4-05e7-4fd4-90d0-038b7ce1a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ai21labs/Jamba-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259d895a-c5af-4031-994d-ceccafb10425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.1.2+cu121\n",
      "transformers 4.40.0.dev0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41325c3d8cd84463b165ddc5381baac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "JambaForCausalLM(\n",
       "  (model): JambaModel(\n",
       "    (embed_tokens): Embedding(65536, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (1): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (2): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (3): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (4): JambaAttentionDecoderLayer(\n",
       "        (self_attn): JambaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (5): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (6): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (7): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (8): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (9): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (10): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (11): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (12): JambaAttentionDecoderLayer(\n",
       "        (self_attn): JambaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (13): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (14): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (15): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (16): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (17): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (18): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (19): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (20): JambaAttentionDecoderLayer(\n",
       "        (self_attn): JambaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (21): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (22): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (23): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (24): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (25): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (26): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (27): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (28): JambaAttentionDecoderLayer(\n",
       "        (self_attn): JambaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (29): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (30): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (experts): ModuleList(\n",
       "            (0): JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "      (31): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm()\n",
       "          (B_layernorm): JambaRMSNorm()\n",
       "          (C_layernorm): JambaRMSNorm()\n",
       "        )\n",
       "        (moe): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x JambaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm()\n",
       "        (pre_moe_layernorm): JambaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): JambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=65536, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"transformers\", transformers.__version__)\n",
    "\n",
    "device_map = \"sequential\"\n",
    "max_memory = {0:\"18GiB\", 1: \"22GiB\"}\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         llm_int8_skip_modules=[\"mamba\", \"router\", \"lm_head\"])\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             attn_implementation=\"flash_attention_2\",\n",
    "                                             device_map=device_map,\n",
    "                                             max_memory=max_memory,\n",
    "                                             quantization_config=quantization_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50bfe52-25dd-4007-8e4b-c5d9eaffa485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 3483152256 12.652861742254942\n",
      "torch.float32 3670016 0.013331660986071896\n",
      "torch.uint8 24041750528 87.33380659675899\n"
     ]
    }
   ],
   "source": [
    "def model_dtypes(model):\n",
    "    # Verifying the datatypes.\n",
    "    dtypes = {}\n",
    "    for name, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total * 100)\n",
    "\n",
    "model_dtypes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1673878a-3eb4-4421-a7f5-f21787faed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='ai21labs/Jamba-v0.1', vocab_size=65536, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|unk|>', 'pad_token': '<|pad|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<|unk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81eff76c-d0ac-4f92-81e6-1ebce44e822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_subject(subject: str) -> str:\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s.strip()\n",
    "\n",
    "def get_pretty_subject(subject: str, lang: str) -> str:\n",
    "    return format_subject({\n",
    "        \"en\": subject,\n",
    "        \"ru\": subcategories_en2ru[subject],  # predefined map\n",
    "    }[lang])\n",
    "\n",
    "def get_prompt_from_dataframes(dev_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                               k: int, test_iloc_idx: int, lang: str, subject: str, conversation_type: str):\n",
    "    assert 0 <= k <= 5\n",
    "    headline_prefix = LANGUAGE_CONFIG[lang][\"headline_prefix\"]\n",
    "    headline_postfix = get_pretty_subject(subject=subject, lang=lang)\n",
    "    headline = f\"{headline_prefix} {headline_postfix}.\\n\\n\"\n",
    "\n",
    "    answer_prefix = LANGUAGE_CONFIG[lang][\"answer_prefix\"]\n",
    "\n",
    "    conv = conversation_classes[conversation_type]()\n",
    "\n",
    "    is_already_taken_headline = False\n",
    "    for row_idx, row in dev_df.head(k).iterrows():\n",
    "        q = row[0].strip()\n",
    "        options = row[1:5].tolist()\n",
    "        lettered_options = [f\"{x}) {y}\" for x, y in zip([\"A\", \"B\", \"C\", \"D\"], options)]\n",
    "        q_with_lettered_options = \"\\n\".join([q, \"\\n\".join(lettered_options)])\n",
    "        if row_idx == 0:\n",
    "            q_with_lettered_options = headline + q_with_lettered_options\n",
    "            is_already_taken_headline = True\n",
    "        conv.append_message(conv.roles[0], q_with_lettered_options)\n",
    "        a = row[5]\n",
    "        \n",
    "        # if is not instruct, needed to be manually separated for mmlu examples\n",
    "        conv.append_message(conv.roles[1], f\"\\n{answer_prefix}{a}\")\n",
    "\n",
    "    row = test_df.iloc[test_iloc_idx]\n",
    "    q = row[0]\n",
    "    options = row[1:5].tolist()\n",
    "    lettered_options = [f\"{x}) {y}\" for x, y in zip([\"A\", \"B\", \"C\", \"D\"], options)]\n",
    "    q_with_lettered_options = \"\\n\".join([q, \"\\n\".join(lettered_options)])\n",
    "    if not is_already_taken_headline:\n",
    "        q_with_lettered_options = headline + q_with_lettered_options\n",
    "        is_already_taken_headline = True\n",
    "    conv.append_message(conv.roles[0], q_with_lettered_options)\n",
    "    a = row[5]\n",
    "    conv.append_message(conv.roles[1], \"\\n\" + answer_prefix)\n",
    "    # prompt = f\"{conv.get_prompt()}{answer_prefix}\"\n",
    "    return conv.messages\n",
    "\n",
    "def calculate_token_interest_probs(\n",
    "    input_ids,\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase,\n",
    "    model: tp.Union[transformers.PreTrainedModel, peft.peft_model.PeftModelForCausalLM],\n",
    ") -> tp.Dict[str, float]:    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits  # shape (batch_size, sequence_length, vocab_size)\n",
    "    next_token_logits = logits[:, -1, :]  # shape (batch_size, vocab_size)\n",
    "\n",
    "    next_token_logits = next_token_logits.flatten()\n",
    "    assert next_token_logits.shape == torch.Size((model.config.vocab_size, ))\n",
    "\n",
    "    next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1).cpu()  # all probs over vocab\n",
    "    # assert torch.isclose(next_token_probs.sum(), torch.tensor(1.0).to(next_token_probs.dtype), atol=1e-03)  # dtype for half/nothalf, -03 for float16\n",
    "    \n",
    "    tokens_of_interest = [\n",
    "        tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "    ]\n",
    "\n",
    "    probs = next_token_probs[tokens_of_interest].tolist()\n",
    "    res = dict(zip([\"A\", \"B\", \"C\", \"D\"], probs))\n",
    "    return res\n",
    "\n",
    "def append_to_jsonl(data: list, filename: str) -> None:\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "def evaluate_subject(\n",
    "    subject: str,\n",
    "    lang: str,\n",
    "    k_shot: int,\n",
    "    jsonl_filepath: str,\n",
    "    maxlen: int,\n",
    "    convtype: str,\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase,\n",
    "    model: tp.Union[transformers.PreTrainedModel, peft.peft_model.PeftModelForCausalLM],\n",
    ") -> None:\n",
    "\n",
    "    dev_df = get_df_in_hendrycks_format(subject=subject, split=\"dev\", lang=lang)\n",
    "    test_df = get_df_in_hendrycks_format(subject=subject, split=\"test\", lang=lang)\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=subject):\n",
    "\n",
    "        current_k_shot = k_shot\n",
    "        skip_too_lengthy = False\n",
    "        while True:\n",
    "            if current_k_shot < 0:\n",
    "                logger.info(\"Skip too lengthy.\")\n",
    "                skip_too_lengthy = True\n",
    "                break\n",
    "            input_messages = get_prompt_from_dataframes(\n",
    "                dev_df=dev_df,\n",
    "                test_df=test_df,\n",
    "                k=current_k_shot,\n",
    "                test_iloc_idx=idx,\n",
    "                lang=lang,\n",
    "                subject=subject,\n",
    "                conversation_type=convtype,\n",
    "            )\n",
    "            input_prompt = tokenizer.apply_chat_template(input_messages, tokenize=False, add_special_tokens=False)[:-len(tokenizer.eos_token)]\n",
    "            input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "            if input_ids.shape[-1] > maxlen and current_k_shot >= 0:\n",
    "                logger.info(\"Takes smaller current_k_shot since maxlen.\")\n",
    "                current_k_shot -= 1\n",
    "            elif current_k_shot < 0:\n",
    "                logger.info(\"Skip too lengthy.\")\n",
    "                skip_too_lengthy = True\n",
    "            else:\n",
    "                break\n",
    "        if skip_too_lengthy:\n",
    "            continue\n",
    "\n",
    "        label = row[5]\n",
    "\n",
    "        preds = calculate_token_interest_probs(\n",
    "            input_ids=input_ids,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        append_to_jsonl(data=[input_prompt, label, preds], filename=jsonl_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e010ee51-51c6-4944-b256-8836a3efe801",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28482116-3503-4333-8a14-d6ba9e919cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '\\n' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{% endif %}{% endfor %}{{ eos_token }}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3935963-d5d7-4b60-b69c-08436870a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lang = \"en\"\n",
    "subject = \"high_school_european_history\"\n",
    "convtype = \"empy_prompt_conv\"\n",
    "current_k_shot = 5\n",
    "idx = 5\n",
    "dev_df = get_df_in_hendrycks_format(subject=subject, split=\"dev\", lang=lang)\n",
    "test_df = get_df_in_hendrycks_format(subject=subject, split=\"test\", lang=lang)\n",
    "\n",
    "input_messages = get_prompt_from_dataframes(\n",
    "    dev_df=dev_df,\n",
    "    test_df=test_df,\n",
    "    k=current_k_shot,\n",
    "    test_iloc_idx=idx,\n",
    "    lang=lang,\n",
    "    subject=subject,\n",
    "    conversation_type=convtype,\n",
    ")\n",
    "input_prompt = tokenizer.apply_chat_template(input_messages, tokenize=False, add_special_tokens=False)[:-len(tokenizer.eos_token)]\n",
    "print(input_prompt)\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=10)\n",
    "ouput_str = tokenizer.decode(output_ids[0])\n",
    "ouput_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e07f7-1edf-4ecf-a86c-ecee3959fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"mmlu_en_Jamba-v0.1_bnb4q\"\n",
    "lang = \"en\"\n",
    "k_shot = 5\n",
    "maxlen = 8192\n",
    "convtype = \"empy_prompt_conv\"\n",
    "\n",
    "subjects = list(subcategories.keys())\n",
    "for each_subject in subjects:\n",
    "    jsonl_filepath = str(pathlib.Path(output_dir) / f\"{each_subject}.jsonl\")\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Filepath JSONL: {jsonl_filepath}\")\n",
    "    if pathlib.Path(jsonl_filepath).exists():\n",
    "        logger.info(f\"File already exists! Please manually verify that it wasn't partially interrupted.\")\n",
    "        continue\n",
    "    evaluate_subject(\n",
    "            subject=each_subject,\n",
    "            lang=lang,\n",
    "            k_shot=k_shot,\n",
    "            jsonl_filepath=jsonl_filepath,\n",
    "            maxlen=maxlen, convtype=convtype,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2990dd5-47ce-408f-a0f6-e8929587e95b",
   "metadata": {},
   "source": [
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f1ad55-2826-45dd-a5e1-874421a37218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "category_to_main_category = {value: key for key, sublist in categories.items() for value in sublist}\n",
    "subcategories2categories = {key: category_to_main_category[value[0]] for key, value in subcategories.items()}\n",
    "\n",
    "def calculate_accuracy_from_directory(dirpath: str) -> tp.Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    assert pathlib.Path(dirpath).exists()\n",
    "    filepaths = [str(x) for x in pathlib.Path(dirpath).glob('*.jsonl')]\n",
    "    # assert len(filepaths) == 57\n",
    "    res = {}\n",
    "    for each_filepath in filepaths:\n",
    "        df = pd.read_json(each_filepath, lines=True)\n",
    "        df.columns = ['prompt', 'label', 'preds']\n",
    "        cors = []\n",
    "        for idx, row in df.iterrows():\n",
    "            preds = row['preds']\n",
    "            best_idx = np.argmax(list(preds.values()))\n",
    "            y_pred = list(preds.keys())[best_idx]\n",
    "            y_true = row['label']\n",
    "            y_pred = y_pred.strip()\n",
    "            y_true = y_true.strip()\n",
    "            cors.append(y_true == y_pred)\n",
    "        acc = np.mean(cors)\n",
    "        res[pathlib.Path(each_filepath).stem] = acc * 100\n",
    "    \n",
    "    df = pd.DataFrame({pathlib.Path(dirpath).stem: res}).reset_index()\n",
    "    df = df.rename(columns={'index': 'subcategory'})\n",
    "    subcategories_df = df.copy()\n",
    "    \n",
    "    df = subcategories_df.copy()\n",
    "    df['subcategory'] = df['subcategory'].map(subcategories2categories)\n",
    "    df = df.rename(columns={'subcategory': 'category'})\n",
    "    df = df.groupby('category').mean().reset_index()\n",
    "    categories_df = df.copy()\n",
    "    \n",
    "    total_df = pd.DataFrame({pathlib.Path(dirpath).stem: [categories_df[pathlib.Path(dirpath).stem].mean()]})\n",
    "    \n",
    "    # assert subcategories_df.shape == (57, 2)\n",
    "    # assert categories_df.shape == (4, 2)\n",
    "    # assert total_df.shape == (1, 1)\n",
    "    return (subcategories_df, categories_df, total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c17f230d-82f6-403e-af38-b9eaf698e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmlu 58.75322164062088\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>mmlu_en_Jamba-v0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>57.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>astronomy</td>\n",
       "      <td>78.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business_ethics</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>73.207547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>college_biology</td>\n",
       "      <td>73.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>college_chemistry</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>college_computer_science</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>college_mathematics</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>college_medicine</td>\n",
       "      <td>60.693642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>college_physics</td>\n",
       "      <td>36.274510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>computer_security</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 subcategory  mmlu_en_Jamba-v0\n",
       "0           abstract_algebra         30.000000\n",
       "1                    anatomy         57.037037\n",
       "2                  astronomy         78.289474\n",
       "3            business_ethics         65.000000\n",
       "4         clinical_knowledge         73.207547\n",
       "5            college_biology         73.611111\n",
       "6          college_chemistry         46.000000\n",
       "7   college_computer_science         49.000000\n",
       "8        college_mathematics         42.000000\n",
       "9           college_medicine         60.693642\n",
       "10           college_physics         36.274510\n",
       "11         computer_security         73.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"mmlu_en_Jamba-v0.1_bnb4q\"\n",
    "\n",
    "subcategories_df, categories_df, total_df = calculate_accuracy_from_directory(dirpath=output_dir)\n",
    "print(\"mmlu\", total_df.values[0][0])\n",
    "subcategories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8598298-6ac2-48d6-9b6e-b8818b29155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmlu 65.21228962312392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>mmlu_en_Mixtral-8x7B-Instruct-v0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>65.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>astronomy</td>\n",
       "      <td>79.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business_ethics</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>77.735849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>college_biology</td>\n",
       "      <td>79.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>college_chemistry</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>college_computer_science</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>college_mathematics</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>college_medicine</td>\n",
       "      <td>72.254335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>college_physics</td>\n",
       "      <td>45.098039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>computer_security</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 subcategory  mmlu_en_Mixtral-8x7B-Instruct-v0\n",
       "0           abstract_algebra                         35.000000\n",
       "1                    anatomy                         65.925926\n",
       "2                  astronomy                         79.605263\n",
       "3            business_ethics                         68.000000\n",
       "4         clinical_knowledge                         77.735849\n",
       "5            college_biology                         79.861111\n",
       "6          college_chemistry                         53.000000\n",
       "7   college_computer_science                         59.000000\n",
       "8        college_mathematics                         42.000000\n",
       "9           college_medicine                         72.254335\n",
       "10           college_physics                         45.098039\n",
       "11         computer_security                         82.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ # revision=\"gptq-4bit-32g-actorder_True\"\n",
    "output_dir = \"mmlu_en_Mixtral-8x7B-Instruct-v0.1-GPTQ_4q32\"\n",
    "\n",
    "subcategories_df, categories_df, total_df = calculate_accuracy_from_directory(dirpath=output_dir)\n",
    "print(\"mmlu\", total_df.values[0][0])\n",
    "subcategories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc90444-619b-442a-bca1-f753fd454438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d55be-dec3-43c1-9409-23cb52f108d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
